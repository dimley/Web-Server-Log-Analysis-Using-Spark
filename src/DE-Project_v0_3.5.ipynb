{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbTR1bgRvjLe"
      },
      "source": [
        "#### DA 231o Data Engineering at Scale (August 2023)\n",
        "# Project : Web Server Log Analysis Using Spark\n",
        "\n",
        "## version: 1\n",
        "\n",
        "### Change Log\n",
        "v0.1, 2023-09-06: Creation of the template\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDtWDLMHlL84"
      },
      "source": [
        "##Abstract:  \n",
        "Web server log analysis can be a potential solution to various problems related to understanding and optimizing web based system and services. Some major problems for which web server analysis can include User Behaviour analysis, performance monitoring, E-commerce optimization, Ad Campaign effectiveness etc. This can also help in server monitoring and taking decision for service scale up/down. Thus, can save from getting startled in case of increased load into server.\n",
        "The goal of the project is to perform comprehensive analysis of any information which contains data from http /http2 URLs which can be from any sector, e-commerce logs, telecom logs, finance logs etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oPswHOwj690"
      },
      "source": [
        "---\n",
        "\n",
        "## Problem Domain and Dataset\n",
        "\n",
        "In Web domain of HTTP based server client architecture, most of the system suffers through different kind of attacks. This attacks can be of differente magnitude ranging from service unavailability, command execution, data leaking and malicious response from the server.\n",
        "One such case we are trying to analyze based on the real world data set available to us. In this project we have tried to manifest the same scenario in telecom domain, where with the progress to 5G architecture this kind of situation is anticipated.\n",
        "\n",
        "Though there are several techniques in the 5G architecture it self to address this issues, but when deployed at private network level still malicious attacks are common threat for any operator.\n",
        "\n",
        "\n",
        "##Dataset\n",
        "\n",
        "We have captured the HTTP based req/response for some of the vital network functions in 5G network. Mostly from different span.\n",
        "Also we have a dataset which contains the suspicious attacks that have been detected at the as per the behavior of that particular device or channel.\n",
        "So, with these dataset we will try to analyze the impact on each network function and segregate different kind of attacks. Also we will manifest the behavior of each network function during these attacks.\n",
        "\n",
        "*** Network function are basically web server.\n",
        "\n",
        "* **Attacks.csv**: Each row in this file contains details about the connections that were made to the servers. Some of the connection are harmless where as few are detected by the ingress controller as malicious. With this dataset we can analyze the impact on different core network functions.\n",
        "```\n",
        "-- ts|uid|id.orig_h|id.orig_p|id.resp_h|id.resp_p|proto|service|duration|orig_bytes|resp_bytes|conn_state|local_orig|local_resp|missed_bytes|history|orig_pkts|orig_ip_bytes|resp_pkts|resp_ip_bytes|tunnel_parents|label|detailed-label: string (nullable = true)\n",
        "```\n",
        "More details are  here: [Any external link]\n",
        "\n",
        "\n",
        "* **server_behavior.csv**: This file contains the HTTP req/res with failure result code from the network functions at several spans. There can be erroneous response from the function based on different cases. But the 500 series of HTTP error code shall be on our focus. It reflects the imapct on the network function out attack like DOS and DDOS.\n",
        " ```|-- No : integer (nullable = true) ```\n",
        "\n",
        " ```|-- Time: string (nullable = true)```\n",
        "\n",
        " ```|-- Source: string (nullable = true)```\n",
        "\n",
        " ```|-- Destination: string (nullable = true)```\n",
        "\n",
        " ```|-- Source Port : string (nullable = true)```\n",
        "\n",
        " ```|-- Destination Port: string (nullable = true)```\n",
        "\n",
        " ```|-- Protocol: string (nullable = true)```\n",
        "\n",
        " ```|-- Length: integer (nullable = true)```\n",
        "\n",
        " ```|-- Status: string (nullable = true)```\n",
        "\n",
        " ```|-- Info: string (nullable = true)```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Enjs-18ywexf"
      },
      "source": [
        "Install Spark/Hadoop and Python dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOG-4wjov462",
        "outputId": "98770958-1470-4f12-c0af-022feab3d1e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "-r-------- 1 root root 220400553 Sep  4 09:38 /content/drive/Shareddrives/DA231-2023/assignments/spark-3.0.3-bin-hadoop2.7.tgz\n"
          ]
        }
      ],
      "source": [
        "#######################################\n",
        "###!@0 START INIT ENVIRONMENT\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "!ls -al /content/drive/Shareddrives/DA231-2023/assignments/spark-3.0.3-bin-hadoop2.7.tgz\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!tar xf /content/drive/Shareddrives/DA231-2023/assignments/spark-3.0.3-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.3-bin-hadoop2.7\"\n",
        "###!@0 END INIT ENVIRONMENT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fkN7_xTz2lU"
      },
      "source": [
        " Standard initialization for the Spark session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "4uoyS0BzhjVp",
        "outputId": "ca216d40-543e-4712-b97a-ac580e977e08"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7c2f102e95d0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://06315b615676:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.0.3</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>MyApp</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "#######################################\n",
        "###!@1 START OF PYSPARK INIT\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "from pyspark.sql import SparkSession\n",
        "input_type = 'sample'\n",
        "spark = SparkSession.builder\\\n",
        "         .master(\"local\")\\\n",
        "         .appName(\"MyApp\")\\\n",
        "         .config('spark.ui.port', '4050')\\\n",
        "         .getOrCreate()\n",
        "spark\n",
        "# Spark is ready to go within Colab!\n",
        "###!@1 END OF PYSPARK INIT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ihkdapK6kOeW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "y2Zyu0iH7LbU"
      },
      "outputs": [],
      "source": [
        "#######################################\n",
        "###!@1 START OF PYSPARK INIT\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n",
        "\n",
        "###!@1 END OF PYSPARK INIT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TQDv4Ye1-dw"
      },
      "source": [
        "### Initialize all the variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plfft8xr1xgm"
      },
      "outputs": [],
      "source": [
        "\n",
        "#######################################\n",
        "###!@2 START OF DEFINING INPUT FILES\n",
        "basepath = \"/content/drive/MyDrive/content/\"\n",
        "Attack_dataset = basepath + \"generated_data.csv\"\n",
        "#Attack_dataset = basepath + \"Attack_details.csv\"\n",
        "Server_status_dataset = basepath + \"server_status.csv\"\n",
        "Maintenance_dataset = basepath + \"maintanence_schedule.csv\"\n",
        "\n",
        "df_StatusData = spark.read.csv(Server_status_dataset, header=True, inferSchema=True)\n",
        "df_AttackData = spark.read.csv(Attack_dataset, header=True, inferSchema=True)\n",
        "df_MaintaData = spark.read.csv(Maintenance_dataset, header=True, inferSchema=True)\n",
        "###!@3 END OF LOADING DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BONQuI9sxSNj"
      },
      "source": [
        "### Common Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWkj6AQdxVAL"
      },
      "outputs": [],
      "source": [
        "#######################################\n",
        "###!@5 START COMMON USER IMPORTS\n",
        "#######################################\n",
        "## Specify valid imports, if any, common to ALL your answers  ==========\n",
        "## start your edits here =================\n",
        "import math\n",
        "import time\n",
        "from datetime import datetime\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
        "from pyspark.sql.functions import *\n",
        "from matplotlib import pyplot as plt\n",
        "import plotly.express as px\n",
        "import numpy as np\n",
        "\n",
        "## end your edits here =============="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_j2OeZ0lL88"
      },
      "source": [
        "### Utility APIs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KV-XXmLCxVK4"
      },
      "outputs": [],
      "source": [
        "#######################################\n",
        "###!@6 START COMMON USER FUNCTIONS\n",
        "#######################################\n",
        "## Specify user defined functions, if any, common to ALL your answers   =====\n",
        "## start your edits here =================\n",
        "\n",
        "\n",
        "def get_labels(AttackCleanedDF):\n",
        "  return AttackCleanedDF.select(\"label\").distinct().rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## end your edits here =================\n",
        "###!@6 END COMMON USER FUNCTIONS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5l_cSWnDrAw"
      },
      "source": [
        "### ---\n",
        "### **1) Data Parsing and Cleaning (DATA Munging)**\n",
        "\n",
        "***Context:*** *Data Science in the wild requires data preparation and cleaning before you can perform exploratory analysis on them. This can include parsing, removing invalid fields, extrapolation, etc. Let's try a flavor of it here.*\n",
        "\n",
        "### **Below is the output for raw data structure from dataset.**\n",
        "## **Attach.CSV**\n",
        " ```\n",
        " |-- timestamp: string (nullable = true)\n",
        " |-- id_orig_h: string (nullable = true)\n",
        " |-- id_orig_p: string (nullable = true)\n",
        " |-- id_resp_h: string (nullable = true)\n",
        " |-- id_resp_p: string (nullable = true)\n",
        " |-- proto: string (nullable = true)\n",
        " |-- detailed-label: string (nullable = true)\n",
        " ```\n",
        "\n",
        "### **Server_behavior.csv**\n",
        " ```\n",
        "|-- No : integer (nullable = true)\n",
        "# |-- Time: string (nullable = true)\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n",
        "\n",
        "|-- Source: string (nullable = true)\n",
        "|-- Destination: string (nullable = true)\n",
        "|-- Source Port : string (nullable = true)\n",
        "|-- Destination Port: string (nullable = true)\n",
        "|-- Protocol: string (nullable = true)\n",
        "|-- Length: integer (nullable = true)\n",
        "|-- Status: string (nullable = true)\n",
        "|-- Info: string (nullable = true)\n",
        "```\n",
        "\n",
        "We need to perform DATA munging activity to clean and formalize the data for further data processing. So below code shall perform DATA munging activity.\n",
        "Activity:\n",
        "    1. Convert Attack.csv into csv.\n",
        "    2. Remove any entries with empty source and destination IP port\n",
        "    3. Convert the timestamp from UNIX format to readable format.\n",
        "    4. Convert the column name to same format. Remove \".\" from column name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBujVoUkE0Gk"
      },
      "outputs": [],
      "source": [
        "#######################################\n",
        "###convert Attack dataset to csv format\n",
        "data = spark.read.text(Attack_dataset)\n",
        "header = data.first().value.split(\"|\")\n",
        "schema = StructType([StructField(name, StringType(), True) for name in header])\n",
        "\n",
        "# Skip the first line (header) and split the remaining lines\n",
        "lines =  data.rdd.zipWithIndex().map(lambda x: x[0].value.split(\"|\")).filter(lambda row: len(row) == len(header))\n",
        "# Create a DataFrame from the cleaned data and schema\n",
        "df = spark.createDataFrame(lines, schema=schema)\n",
        "print(df.count())\n",
        "\n",
        "##convert the  ts fields from unix timestamp to readable format.\n",
        "df = df.withColumn(\"timestamp\", from_unixtime(\"ts\"))\n",
        "df = df.drop(\"ts\")\n",
        "\n",
        "##remove \".\" from the coulmn names\n",
        "new_column_names = []\n",
        "for column_name in df.columns:\n",
        "    new_column_name = column_name.replace(\".\", \"_\")\n",
        "    new_column_names.append(new_column_name)\n",
        "\n",
        "#Rename the columns\n",
        "for i, old_name in enumerate(df.columns):\n",
        "    df = df.withColumnRenamed(old_name, new_column_names[i])\n",
        "\n",
        "# Reorder the columns to have \"timestamp\" at the beginning\n",
        "column_order = [\"timestamp\"] + [col_name for col_name in df.columns if col_name != \"timestamp\"]\n",
        "df =df.select(column_order)\n",
        "\n",
        "###Select the relevant column for our operation\n",
        "AttackCleanedDF = df.select(\"timestamp\", \"id_orig_h\", \"id_orig_p\", \"id_resp_h\", \"id_resp_p\", 'proto', 'duration', 'label')\n",
        "#AttackCleanedDF.show()\n",
        "#AttackCleanedDF.printSchema()\n",
        "\n",
        "## No cleaning is required for Server_Details and Maintenance_schedule dataset\n",
        "\n",
        "## end your edits here ================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRnuAsRSE0jL"
      },
      "source": [
        "### **2) Removal of BENIGN only sources from the data set**\n",
        "a). Check if there is any source which later on turned into an attacker.\n",
        "\n",
        "b). Remove sources with no attack from the dataset. Keep source with both (BENIGN and MALICIOUS) or only MALICIOUS role.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WT1LIoXPE2qY"
      },
      "outputs": [],
      "source": [
        "#AttackCleanedDF.persist()\n",
        "#AttackSampleDF = AttackCleanedDF.sample(False, 1.0)\n",
        "#AttackCleanedDF.show(10)#select('id_orig_h').distinct().show(10)\n",
        "#StatusSample = df_StatusData.sample(False, 0.1)\n",
        "#df_StatusData.persist()\n",
        "#AttackCleanedDF.show()\n",
        "#df_StatusData.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmWxjvXcOmfM"
      },
      "source": [
        "\n",
        "##In this section we will see three types of sources\n",
        "1. Pure Benign Source\n",
        "2. Pure Malicious Source\n",
        "3. Mixed Source\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsMRPhxvN0-t"
      },
      "outputs": [],
      "source": [
        "!pip install -U kaleido\n",
        "import plotly.io as pio\n",
        "\n",
        "def getBenignSources(AttackCleanedDF):\n",
        "  BenignSources = AttackCleanedDF.filter(AttackCleanedDF[\"label\"].startswith('B')).select(\"id_orig_h\").distinct()\n",
        "  return BenignSources\n",
        "def getMaliciousSources(AttackCleanedDF):\n",
        "  MaliciousSources = AttackCleanedDF.filter(AttackCleanedDF[\"label\"].startswith('M')).select(\"id_orig_h\").distinct()\n",
        "  return MaliciousSources\n",
        "def getMixedSources(BenignSources, MaliciousSources):\n",
        "  BenignMaliciousSources = BenignSources.join(MaliciousSources,BenignSources.id_orig_h ==  MaliciousSources.id_orig_h,\"inner\")\\\n",
        "                                        .drop(MaliciousSources.id_orig_h)\\\n",
        "                                        .groupBy(\"id_orig_h\").count().select(\"id_orig_h\")\n",
        "  return BenignMaliciousSources\n",
        "def plotAllSourceType(data,valueCol,nameCol,title='Title'):\n",
        "  fig = px.pie(data.toPandas().to_dict(), values=valueCol, names=nameCol, title=title)\n",
        "  fig.show()\n",
        "  #pio.write_image(fig, \"figname.png\")\n",
        "  #fig.save()\n",
        "\n",
        "\n",
        "BenignSources = getBenignSources(AttackCleanedDF)\n",
        "MaliciousSources = getMaliciousSources(AttackCleanedDF)\n",
        "MixedSources = getMixedSources(BenignSources, MaliciousSources)\n",
        "PureBenignSources = BenignSources.subtract(MixedSources).withColumn(\"type\", lit(\"Pure Benign\"))\n",
        "PureMaliciousSources = MaliciousSources.subtract(MixedSources).withColumn(\"type\", lit(\"Pure Malicious\"))\n",
        "MixedTypeSources = MixedSources.withColumn(\"type\", lit(\"Mixed\"))\n",
        "\n",
        "#MixedTypeSources.show()\n",
        "#PureMaliciousSources.show()\n",
        "#MixedTypeSources.show()\n",
        "\n",
        "UserTypeDf = MixedTypeSources.union(PureMaliciousSources).union(PureBenignSources)\n",
        "UserTypeDf.show()\n",
        "\n",
        "UserTypeCntDf = UserTypeDf.groupBy('type').count()\n",
        "UserTypeCntDf.show()\n",
        "\n",
        "plotAllSourceType(UserTypeCntDf,'count','type','Type Of Users')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEQvVf6vyGPN"
      },
      "outputs": [],
      "source": [
        "def plotMixedDFwithTime(MixedCleanedDF,BCleanedDF):\n",
        "  #plot for 24 hour buckets\n",
        "  d = MixedCleanedDF.toPandas().to_dict()\n",
        "  b = BCleanedDF.toPandas().to_dict()\n",
        "\n",
        "  hours = np.arange(24)\n",
        "  benign = [int(0) for _ in range(24)]\n",
        "  malicious = [int(0) for _ in range(24)]\n",
        "  pb = [int(0) for _ in range(24)]\n",
        "  width = 0.4\n",
        "  for i in range(24):\n",
        "    if i in d['count'].keys():\n",
        "      count = d['count'][i]\n",
        "      category = d['typeMB'][i]\n",
        "      hour = d['hour'][i]\n",
        "      if category == 'M':\n",
        "        malicious[hour] += count\n",
        "      else:\n",
        "        benign[hour] += count\n",
        "  for i in range(24):\n",
        "    if i in b['count'].keys():\n",
        "      count = b['count'][i]\n",
        "      category = b['typeMB'][i]\n",
        "      hour = b['hour'][i]\n",
        "      if category == 'B':\n",
        "        pb[hour] += count\n",
        "  plt.bar(hours, malicious, width, label=\"Malicious\")\n",
        "  plt.bar(hours+width, benign, width, label=\"False Benign\")\n",
        "  #plt.bar(hours+2*width, pb, width, label=\"True Users\")\n",
        "  plt.title(\"Behaviour of Attackers Classified as Benign\")\n",
        "  plt.xticks(hours)\n",
        "  plt.xlabel(\"Hour\")\n",
        "  plt.ylabel(\"Number of requests\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "#Create MaliciousDF and MixedDF\n",
        "OnlyMixedTypeDF = UserTypeDf.filter(UserTypeDf[\"type\"] == \"Mixed\")\n",
        "MixedDF = AttackCleanedDF.join(OnlyMixedTypeDF,AttackCleanedDF.id_orig_h == OnlyMixedTypeDF.id_orig_h,\"inner\").drop(OnlyMixedTypeDF.id_orig_h)\n",
        "#MixedDF.show()\n",
        "\n",
        "#append hour column\n",
        "MixedDFWithTime = MixedDF.withColumn(\"hour\", hour(MixedDF.timestamp)).withColumn(\"typeMB\",substring(MixedDF.label,0,1))\n",
        "#MixedDFWithTime.show()\n",
        "\n",
        "MixedCleanedDF = MixedDFWithTime.select(\"id_orig_h\",\"typeMB\",\"hour\").groupBy(\"typeMB\",\"hour\").count()\n",
        "MixedCleanedDF.show()\n",
        "\n",
        "#Create MaliciousDF and MixedDF\n",
        "OnlyBenignTypeDF = UserTypeDf.filter(UserTypeDf[\"type\"] == \"Pure Benign\")\n",
        "BDF = AttackCleanedDF.join(OnlyBenignTypeDF,AttackCleanedDF.id_orig_h == OnlyBenignTypeDF.id_orig_h,\"inner\").drop(OnlyBenignTypeDF.id_orig_h)\n",
        "#MixedDF.show()\n",
        "\n",
        "#append hour column\n",
        "BDFWithTime = BDF.withColumn(\"hour\", hour(BDF.timestamp)).withColumn(\"typeMB\",substring(BDF.label,0,1))\n",
        "#MixedDFWithTime.show()\n",
        "\n",
        "BCleanedDF = BDFWithTime.select(\"id_orig_h\",\"typeMB\",\"hour\").groupBy(\"typeMB\",\"hour\").count()\n",
        "BCleanedDF.show()\n",
        "\n",
        "plotMixedDFwithTime(MixedCleanedDF,BCleanedDF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfB_10FzTqGV"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "dt = datetime.now()\n",
        "import time\n",
        "time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_8PVpZwE3P1"
      },
      "source": [
        "### **3) Identify all categories of attack from dataset.**\n",
        "\n",
        "   a) Create pie chart to depict the categorization and percentage of these attcak.( number of occurrance)\n",
        "\n",
        "   B) Create pie chart to depict the categorization and percentage of these attcak.(Duration)\n",
        "\n",
        "   c) Create pie chart to depict the categorization and percentage of these attcak.(Bandwidth no of packets)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRNbzyS8OcqR"
      },
      "source": [
        "### We can see 4 types of Attacks in our systems.\n",
        "1. Malicious   PartOfAHorizontalPortScan\n",
        "2. Malicious   C&C\n",
        "3. Malicious   DDoS\n",
        "4. Malicious"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-O1EjQdE4G3"
      },
      "outputs": [],
      "source": [
        "groupedMaliciousDF = AttackCleanedDF.filter(AttackCleanedDF.label.contains(\"Malicious\")).groupBy(\"label\").count()\n",
        "groupedMaliciousDF.show()\n",
        "\n",
        "plotAllSourceType(groupedMaliciousDF,'count','label','Types Of Attack')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0L5GjiDRIDy"
      },
      "source": [
        "**Conclusion**\n",
        "- More than half of the attacks are DDoS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWcnj_20TFW0"
      },
      "source": [
        "## **Category wise Average Duration of Attacks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dkAFyLBSFN7"
      },
      "outputs": [],
      "source": [
        "groupedMaliciousDurationDF = AttackCleanedDF.filter(AttackCleanedDF.label.contains(\"Malicious\"))\\\n",
        "                                            .filter(~AttackCleanedDF.duration.contains(\"-\"))\\\n",
        "                                            .groupBy(\"label\").agg(avg(\"duration\").alias(\"avg_duration\"))\n",
        "#groupedMaliciousDurationDF.show()\n",
        "\n",
        "plotAllSourceType(groupedMaliciousDurationDF,'avg_duration','label','Average Duration of Attacks')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXwrA__wVOCj"
      },
      "source": [
        "# **Summary**\n",
        "- All Types Of Attacks have similar Duration. They are lasting for Avg 3 Min Duration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uh_A5A1pF6KU"
      },
      "source": [
        "\n",
        "\n",
        "### **4. Analyze the server behavior during the time of attack.**\n",
        "\n",
        "   a) Categorize diferent kind of servers from the logs.\n",
        "\n",
        "   b) Create pie chart to depict the impact caused by different types of attack on these servers.\n",
        "\n",
        "   c) Which server was mostly impacted because of these attack?\n",
        "\n",
        "   D) Analyze the type of imapct casued in these servers using HTTP return code.\n",
        "\n",
        "   E) capture the behavior in Pie chart or similar diagram.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZCmNaC0GM0Z"
      },
      "outputs": [],
      "source": [
        "#df_StatusData.show()\n",
        "df_copy = df_StatusData.alias(\"df_copy\")\n",
        "\n",
        "def categorize_info(info):\n",
        "    return when(info.contains('nausf-auth'), 'AUSF') \\\n",
        "           .when(info.contains('nudm-sdm'), 'UDM') \\\n",
        "           .when(info.contains('npcf-smpolicy'), 'PCF') \\\n",
        "           .otherwise('Responses')\n",
        "\n",
        "# Apply the categorize_info function to create a new 'Category' column\n",
        "df_copy = df_copy.withColumn('server_type', categorize_info(col('Info')))\n",
        "# Count the occurrences for each category\n",
        "category_counts = df_copy.groupBy('server_type').agg(count('*').alias('Count'))\n",
        "\n",
        "# Filter out rows with 'Other' category\n",
        "category_counts = category_counts.filter(col('server_type').isin('AUSF', 'UDM', 'PCF'))\n",
        "# Show the resulting counts\n",
        "#category_counts.show()\n",
        "\n",
        "#df_copy.show(truncate=False)\n",
        "\n",
        "#create a pie chart\n",
        "import matplotlib.pyplot as plt\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "\n",
        "def draw_pie_chart(pd_df, column_name, title):\n",
        "    pd_df = category_counts.toPandas()\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.pie(pd_df['Count'], labels=pd_df[column_name], autopct='%1.1f%%', startangle=140)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "draw_pie_chart(category_counts, 'server_type', 'Failure Distribution')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df_copy = df_StatusData.alias(\"df_copy\")\n",
        "\n",
        "def categorize_info(info):\n",
        "    return when(info.contains('500'), '500') \\\n",
        "           .when(info.contains('501'), '501') \\\n",
        "           .when(info.contains('502'), '502') \\\n",
        "           .when(info.contains('503'), '503') \\\n",
        "           .when(info.contains('511'), '511') \\\n",
        "           .when(info.contains('5'), 'HTTP ERROR') \\\n",
        "           .otherwise('Request')\n",
        "\n",
        "# Apply the categorize_info function to create a new 'Category' column\n",
        "df_copy = df_copy.withColumn('error_type', categorize_info(col('Info')))\n",
        "# Count the occurrences for each category\n",
        "category_counts = df_copy.groupBy('error_type').agg(count('*').alias('Count'))\n",
        "\n",
        "\n",
        "# Filter out rows with 'Other' category\n",
        "category_counts = category_counts.filter(col('error_type') != 'Request')\n",
        "draw_pie_chart(category_counts, 'error_type', 'Error Distribution')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUJ4U9KS0FHi"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "# Create a feature vector\n",
        "\n",
        "OnlyMTypeDF = UserTypeDf.filter(UserTypeDf[\"type\"] == \"Pure Malicious\")\n",
        "MDF = AttackCleanedDF.join(OnlyMTypeDF,AttackCleanedDF.id_orig_h == OnlyMTypeDF.id_orig_h,\"inner\").drop(OnlyMTypeDF.id_orig_h)\n",
        "\n",
        "categorical_columns = [\"id_orig_h\", \"id_orig_p\", \"id_resp_h\", \"id_resp_p\",\"proto\"]\n",
        "# Create StringIndexer instances for each column\n",
        "string_indexers = StringIndexer(inputCol=\"id_orig_p\", outputCol=\"id_orig_p_num\")\n",
        "indexed_df = string_indexers.fit(MDF).transform(MDF)\n",
        "indexed_df = indexed_df.drop(\"id_orig_p\")\n",
        "\n",
        "\n",
        "string_indexers = StringIndexer(inputCol=\"id_orig_h\", outputCol=\"id_orig_h_num\")\n",
        "indexed_df = string_indexers.fit(indexed_df).transform(indexed_df)\n",
        "indexed_df = indexed_df.drop(\"id_orig_h\")\n",
        "\n",
        "string_indexers = StringIndexer(inputCol=\"id_resp_h\", outputCol=\"id_resp_h_num\")\n",
        "indexed_df = string_indexers.fit(indexed_df).transform(indexed_df)\n",
        "indexed_df = indexed_df.drop(\"id_resp_h\")\n",
        "\n",
        "string_indexers = StringIndexer(inputCol=\"id_resp_p\", outputCol=\"id_resp_p_num\")\n",
        "indexed_df = string_indexers.fit(indexed_df).transform(indexed_df)\n",
        "indexed_df = indexed_df.drop(\"id_resp_p\")\n",
        "\n",
        "string_indexers = StringIndexer(inputCol=\"proto\", outputCol=\"proto_num\")\n",
        "indexed_df = string_indexers.fit(indexed_df).transform(indexed_df)\n",
        "indexed_df = indexed_df.drop(\"proto\")\n",
        "\n",
        "string_indexers = StringIndexer(inputCol=\"label\", outputCol=\"label_num\")\n",
        "indexed_df = string_indexers.fit(indexed_df).transform(indexed_df)\n",
        "indexed_df = indexed_df.drop(\"label\")\n",
        "\n",
        "# Show the result\n",
        "indexed_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13AgFw2e5yr5"
      },
      "outputs": [],
      "source": [
        "indexed_df.show()\n",
        "indexed_df.printSchema()\n",
        "indexed_df = indexed_df.filter(indexed_df.duration != 'duration')\n",
        "indexed_df.select('id_orig_p_num').show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indexed_df.select('id_orig_p_num').distinct().show()\n",
        "indexed_df.select('id_orig_h_num').distinct().show()\n",
        "indexed_df.select('id_resp_p_num').distinct().show()\n",
        "indexed_df.select('id_resp_p_num').distinct().show()\n",
        "indexed_df.select('proto_num').distinct().show()"
      ],
      "metadata": {
        "id": "xAtSB-SzTkBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGwAjOsd2QJE"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.ml.linalg import DenseVector, SparseVector, Vectors, VectorUDT\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create a feature vector\n",
        "feature_cols = [\"id_orig_h_num\", \"id_orig_p_num\", \"id_resp_h_num\", \"id_resp_p_num\", 'proto_num']\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "indexed_df1 = assembler.transform(indexed_df)\n",
        "\n",
        "\n",
        "toDense = lambda v: Vectors.dense(v.toArray())\n",
        "toDenseUdf = F.udf(toDense, VectorUDT())\n",
        "indexed_df1 = indexed_df1.withColumn('features', toDenseUdf('features'))\n",
        "indexed_df1.select('features').show()\n",
        "\n",
        "# Split the data into training and test sets (80% training, 20% testing)\n",
        "(training_data, test_data) = indexed_df1.randomSplit([0.7, 0.3], seed=123)\n",
        "\n",
        "training_data.printSchema()\n",
        "\n",
        "# Initialize the Random Forest classifier\n",
        "rf = RandomForestClassifier(labelCol=\"label_num\", featuresCol=\"features\", numTrees=10)\n",
        "\n",
        "# Create a pipeline\n",
        "pipeline = Pipeline(stages=[rf])\n",
        "\n",
        "# Train the model\n",
        "model = pipeline.fit(training_data)\n",
        "# Make predictions on the test data\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "# Evaluate the model\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label_num\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdXwn7OmGNIq"
      },
      "source": [
        "### **5. Capture the outliers from the Server dataset and find the impact on these due maintenance operation at network level.**\n",
        "``` Maintenance Dataset contains the operation details executed on these servers by operator.```\n",
        "\n",
        "```TODO capture the fields of Maintenance dataset```\n",
        "\n",
        "  a). Capture if there is any instance when server performance deteriorated, But the is no attack on it.\n",
        "\n",
        "  b). Verify if the poor performance can be attributed to the scheduled maintenance from OPERATIONS.csv dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5szTdtwyGTLA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW_TLGFoGTYn"
      },
      "source": [
        "### **6. Benefit of using protection mechanism like Authentication at Network level**\n",
        "a) Check if any server able to identify and reject the attack request. (HTTP response with 511 Network Authentication Required)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2S6hXvlaq8Me"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}